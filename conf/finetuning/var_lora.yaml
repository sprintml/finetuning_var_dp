name: var_lora
training_params:
  n_gradient_accumulation: 8
  opt: adamw # Optimizer choices: ["adam", "adamw"]
  label_smooth: 0.0
  lr: 5e-4
  grad_clip: 0.1
  weight_decay: 0.05
  rank: 16 # LoRA Alpha = Rank * 2 (Auto set)
  target_modules: ["mat_qkv", "proj", "fc1", "fc2", "ada_lin.1"]
  lora_dropout: 0.0
